# Azure-Fabric-Unified-Data-Engineering-Project
ğŸ“Œ Project Overview
This project implements a complete end-to-end Lakehouse data engineering pipeline using Microsoft Fabric, following the Medallion Architecture design pattern. Incremental data ingestion is handled using Autoloader, 
which efficiently loads new and modified files into the Bronze layer as raw Delta tables, ensuring scalable and optimized processing. In the Silver layer, data is cleaned, standardized, and enriched using PySpark
transformations, including lookup-based joins to integrate reference datasets and maintain data consistency. The Gold layer is designed using a Star Schema model, creating fact and dimension tables optimized for 
analytics and reporting. The entire pipeline is orchestrated using Microsoft Fabric Workflows, enabling automated, parameterized, and scalable execution for enterprise-level batch and near real-time data processing.
The solution implements:
â€¢ Incremental Data Ingestion using Autoloader
â€¢ Lookup-based Data Enrichment
â€¢ PySpark Transformations
â€¢ Medallion Architecture (Bronze â†’ Silver â†’ Gold)
â€¢ Star Schema Modeling
â€¢ Workflow Orchestration
â€¢ Delta Table Optimization

ğŸ“¸Screenshot
![image alt](https://github.com/Charvi-M-J/Azure-Fabric-Unified-Data-Engineering-Project/blob/a0f1deab42c20afed7ef1f414464deb479ea641f/screenshot/Screenshot%202026-02-13%20230253.png)

ğŸ— Architecture Overview
The pipeline follows Medallion Architecture:
  Source Files (CSV / Incremental Data)
        â†“
 Autoloader (Incremental Processing)
        â†“
 Bronze Layer (Raw Delta Tables)
        â†“
 Silver Layer (Cleaned + Lookup Enriched Data)
        â†“
 Gold Layer (Star Schema - Fact & Dimension Tables)
        â†“
 Fabric Warehouse / Power BI Reporting
 
ğŸ› ï¸ Technologies Used:
ğŸ”¹ Microsoft Fabric â€“ Unified platform for end-to-end data engineering and analytics
ğŸ”¹ Lakehouse Architecture â€“ Implements Medallion (Bronze, Silver, Gold) layered design
ğŸ”¹ PySpark â€“ Data transformation, cleaning, and enrichment logic
ğŸ”¹ Apache Spark â€“ Distributed big data processing engine
ğŸ”¹ Delta Tables â€“ ACID-compliant storage with optimized performance
ğŸ”¹ Autoloader â€“ Incremental file ingestion and scalable data loading
ğŸ”¹ Fabric Workflows â€“ Pipeline orchestration and automated execution

ğŸ¯ Key Learnings
â€¢ Designing scalable Lakehouse architectures using Medallion (Bronze, Silver, Gold) design
â€¢ Implementing incremental data ingestion using Autoloader for efficient file processing
â€¢ Configuring and optimizing Autoloader for scalable and near real-time data loads
â€¢ Performing distributed data transformations using Apache Spark and PySpark
â€¢ Applying lookup-based data enrichment to maintain referential integrity
â€¢ Creating business-ready Star Schema models for analytics and reporting
